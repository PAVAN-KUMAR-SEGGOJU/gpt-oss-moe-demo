# GPT-OSS Mixture of Experts (MoE) Demo

This project demonstrates a Mixture of Experts (MoE) approach using GPT-OSS series models. It includes a minimal MoE wrapper, example usage, and visualizations of expert selection per token.

## Features

- Minimal MoE implementation for GPT-OSS models
- Example scripts showing expert/layer selection per token
- Visualization of which experts are selected for each token
- Ready-to-run and GitHub-deployable

## Getting Started

1. **Clone this repository**
2. **Install requirements:**
   ```bash
   pip install -r requirements.txt
